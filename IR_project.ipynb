{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR project Nicholas Capek.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfQo2UKpZ9jT"
      },
      "source": [
        "# Term Project\n",
        "“The Cranfield collection [...] was the pioneering test collection in allowing CRANFIELD precise quantitative measures of information retrieval effectiveness [...]. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs.” [1, Section 8.2]\n",
        "\n",
        "Your tasks, reviewed by your colleagues and the course instructors, are the following:\n",
        "\n",
        "1.   *Implement a ranked retrieval system*, [1, Chapter 6] which will produce a list of documents from the Cranfield collection in a descending order of relevance to a query from the Cranfield collection. You MUST NOT use relevance judgements from the Cranfield collection in your information retrieval system. Relevance judgements MUST only be used for the evaluation of your information retrieval system.\n",
        "\n",
        "2.   *Document your code* in accordance with [PEP 257](https://www.python.org/dev/peps/pep-0257/), ideally using [the NumPy style guide](https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard) as seen in the code from exercises.  \n",
        "     *Stick to a consistent coding style* in accordance with [PEP 8](https://www.python.org/dev/peps/pep-0008/).\n",
        "\n",
        "3.   *Reach at least 35% mean average precision* [1, Section 8.4] with your system on the Cranfield collection. You are encouraged to use techniques for tokenization, [1, Section 2.2] document representation [1, Section 6.4], tolerant retrieval [1, Chapter 3], relevance feedback and query expansion, [1, Chapter 9] and others discussed in the course.\n",
        "\n",
        "4.   *Upload a link to your Google Colaboratory document to the homework vault in IS MU.* You MAY also include a brief description of your information retrieval system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r15UxDfcrpSx"
      },
      "source": [
        "#### Install the fresh version of utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inUAfc6TQMVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd113b1-f42a-4648-c1ca-f1835104d12b"
      },
      "source": [
        "! pip install git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master | grep '^Successfully'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Running command git clone -q https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git /tmp/pip-req-build-hyhdxteb\n",
            "Successfully built gdown\n",
            "Successfully built pv211-utils\n",
            "Successfully installed gdown-3.12.2 pv211-utils-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmpR8qpTZwyP"
      },
      "source": [
        "## Loading the Cranfield collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y845E0ePZqeH"
      },
      "source": [
        "### Loading the documents\n",
        "The following code loads documents from the Cranfield collection into the `documents` [ordered dictionary](https://docs.python.org/3.8/library/collections.html#collections.OrderedDict). Tokenization of the `title` and `body` attributes of the individual documents as well as the creative use of the `authors`, `bibliography`, and `title` attributes is left to your imagination and craftsmanship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyAqWIQyINng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c319ef0b-bd4b-4db4-d341-4ee189a232ac"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re, string\n",
        "import inflect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqQo0S__CIN"
      },
      "source": [
        "def ending_cleaner(text, ending):\n",
        "  \"\"\"Cuts off the end of a string if it ends in ending\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  test\n",
        "      The text which is to be cleaned\n",
        "\n",
        "  ending\n",
        "      The text ending which is to be removed if present\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  text : string\n",
        "      The preprocessed version of text\n",
        "\n",
        "  \"\"\"      \n",
        "  if len(text)>0:\n",
        "    if text.endswith(ending):\n",
        "      return text[:-len(ending)]\n",
        "\n",
        "  return text  \n",
        "\n",
        "def preproces_text(text):\n",
        "  \"\"\"Performs a series of preprocessing tasks on text\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text\n",
        "      The text which is to be cleaned in string format\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  proces : list\n",
        "      A list of preprocessed tokenized version of text\n",
        "\n",
        "  \"\"\"\n",
        "  #make a copy of text     \n",
        "  text_copy = copy.deepcopy(text)\n",
        "  # strip numbers\n",
        "  text_copy = re.sub('[0-9]+', '', text_copy)\n",
        "  # tokenize text\n",
        "  proces = WordPunctTokenizer().tokenize(text_copy)\n",
        "  # convert to lower case\n",
        "  proces = list(map(lambda x: x.lower(), proces))\n",
        "  # remove punctuation\n",
        "  proces = list(map(lambda x: re.sub(r'[^\\w\\s]', '', x), proces))\n",
        "  proces = [token for token in proces if token != '']\n",
        "  # use ending cleaner with ending 'eous', since the word gaseous does not get correctly stemmed\n",
        "  proces = [ending_cleaner(token, 'eous') for token in proces]\n",
        "  # lemmatize the tokens\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  proces = [lemmatizer.lemmatize(token) for token in proces]\n",
        "  # build a list neg_stopwords, which I want included, but are present in nltk stopword list\n",
        "  neg_stopwords = ['must'] \n",
        "  # copy nltk stopwords\n",
        "  nltk_stopwords = [token for token in nltk.corpus.stopwords.words('english')]\n",
        "  # add neg_stopwords to nltk_stopwords\n",
        "  actual_stopwords = list(set(nltk_stopwords) - set(neg_stopwords))  \n",
        "  # add some custom stopwords which I determined should be removed while examining query and document bodies\n",
        "  custom_stopwords = ['viz', 'fig', 'ie', 'eg', 'see', 'dash', 'far', 'one', 'else', 'anyone', 'made', 'way', 'exist']\n",
        "  custom_stopwords.extend(['available', 'done', 'play', 'within', 'work', 'paper', 'quite', 'title','efforts','efforts','chart'])\n",
        "  custom_stopwords.extend(['best', 'done', 'year', 'called', 'mention', 'pre', 'subscript', 'let'])\n",
        "  # remove stopwords from proces list\n",
        "  proces = [token for token in proces if token not in actual_stopwords and token not in custom_stopwords]\n",
        "  # stemming\n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  # prevent 'gas' from being stemmed, since diffenent version of the word don't get stemmed equally\n",
        "  non_stem_words = ['gas']\n",
        "  new_proces = []\n",
        "  for token in proces:\n",
        "    if len(token) > 3:\n",
        "      new_proces.append(stemmer.stem(token))\n",
        "    else:\n",
        "      new_proces.append(token)\n",
        "  proces = new_proces\n",
        "  # remove tokens which are too short\n",
        "  proces = [token for token in proces if len(token) > 2]\n",
        "  return proces\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k18X4JyIfnSw"
      },
      "source": [
        "from pv211_utils.entities import DocumentBase\n",
        "\n",
        "class Document(DocumentBase):\n",
        "    \"\"\"\n",
        "    A Cranfield collection document.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    document_id : int\n",
        "        A unique identifier of the document.\n",
        "    authors : list of str\n",
        "        A unique identifiers of the authors of the document.\n",
        "    bibliography : str\n",
        "        The bibliographical entry for the document.\n",
        "    title : str\n",
        "        The title of the document.\n",
        "    body : str\n",
        "        The abstract of the document.\n",
        "    preprocessed_title : list\n",
        "        Preprocessed version of document title\n",
        "    preprocessed_body : list\n",
        "        Preprocessed version of document body\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, document_id, authors, bibliography, title, body):\n",
        "        super().__init__(document_id, authors, bibliography, title, body)\n",
        "        self.preprocessed_title = preproces_text(title)\n",
        "        self.preprocessed_body = preproces_text(body)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfRrW7O6U5wb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "33d4b89a-b330-4400-b5ae-e124451ba7f8"
      },
      "source": [
        "from pv211_utils.loader import load_documents\n",
        "documents = load_documents(Document)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1e4218e4b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpv211_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pv211_utils.loader'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwYwHs-MpD1_"
      },
      "source": [
        "### Loading the queries\n",
        "The following code loads queries from the Cranfield collection into the `queries` [ordered dictionary](https://docs.python.org/3.8/library/collections.html#collections.OrderedDict). Tokenization of the `body` attribute of the individual queries is left to your imagination and craftsmanship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaCFkMFdKjST"
      },
      "source": [
        "from pv211_utils.entities import QueryBase\n",
        "\n",
        "class Query(QueryBase):\n",
        "    \"\"\"\n",
        "    A Cranfield collection query.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    query_id : int\n",
        "        A unique identifier of the query.\n",
        "    body : str\n",
        "        The text of the query.\n",
        "    preprocessed_body : list\n",
        "        Preprocessed version of query body\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, query_id, body):\n",
        "        super().__init__(query_id, body)\n",
        "        self.preprocessed_body = preproces_text(body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qcyQUNRqRTr"
      },
      "source": [
        "from pv211_utils.loader import load_queries\n",
        "queries = load_queries(Query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mi4WPDUw01w"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T80oQdZGsh6_"
      },
      "source": [
        "### Loading the relevance judgements\n",
        "The following code loads relevance judgements from the Cranfield collection into the `relevant` set. Relevance judgements MUST NOT be used in your information retrieval system. Relevance judgements MUST only be used for the evaluation of your information retrieval system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkdR9X6shQ8"
      },
      "source": [
        "from pv211_utils.loader import load_judgements\n",
        "relevant = load_judgements(queries, documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoCTkRKGVqT6"
      },
      "source": [
        "def uniq(sorted_list):\n",
        "    \"\"\"A sorted list with duplicates removed.\n",
        "    This code is borrowed from week 1 and 2 class notebook\n",
        "    Used to build inverted index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sorted_list : list\n",
        "        The sorted list.\n",
        "  \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The sorted list with duplicates removed.\n",
        "\n",
        "    \"\"\"\n",
        "    if len(sorted_list) <= 1:\n",
        "        return sorted_list\n",
        "    uniq_list = sorted_list[:1]\n",
        "    previous_value = sorted_list[0]\n",
        "    for value in sorted_list[1:]:\n",
        "        if value != previous_value:\n",
        "            uniq_list.append(value)\n",
        "        previous_value = value\n",
        "    return uniq_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJM9TfbEPCZV"
      },
      "source": [
        "## Implementation of your information retrieval system\n",
        "The following code provides an example implementation of an information retrieval system in the `search` function. This example implementation returns documents in a random order and achieves a very weak mean average precision. Replace this implementation with your own implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqq_dijKRT-F"
      },
      "source": [
        "from random import seed, shuffle\n",
        "from collections import OrderedDict\n",
        "\n",
        "from pv211_utils.irsystem import IRSystem\n",
        "\n",
        "class MyIRSystem(IRSystem):\n",
        "    \"\"\"\n",
        "    A system that returns documents in random order.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    random_documents : list of Document\n",
        "        Documents in random order.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold = 0):\n",
        "        self.all_documents = documents\n",
        "        self.output_documents = [] # menim z [all_documents[0]]\n",
        "        self.build_inverted_index()\n",
        "        self.build_tf_idf_frames()\n",
        "        #print(self.output_documents.body)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def build_inverted_index(self):\n",
        "      '''\n",
        "      Builds an inverted index out of all document bodies\n",
        "      as well as documents titles and query bodies\n",
        "      Basic structure is borrowed from week 1 and 2 class notebook\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      global dictionary : list\n",
        "          The list of all unique terms in preprocessed document bodies.\n",
        "      \n",
        "      global inverted_index : dictionary\n",
        "          The dictionary of inverted indexes for each term.\n",
        "\n",
        "      global dictionary_titles : list\n",
        "          The list of all unique terms in preprocessed document titles.\n",
        "      \n",
        "      global inverted_index_titles : dictionary\n",
        "          The dictionary of inverted indexes for each term.\n",
        "\n",
        "      global query_dictionary : list\n",
        "          The list of all unique terms in preprocessed query bodies.\n",
        "      \n",
        "      global query_inverted_index : dictionary\n",
        "          The dictionary of inverted indexes for each term.\n",
        "\n",
        "      '''\n",
        "\n",
        "      # build token, document pairs from preprocessed document bodies\n",
        "      pairs = []\n",
        "      for doc_id in documents:\n",
        "          tokens = documents[doc_id].preprocessed_body\n",
        "          for token in tokens:\n",
        "              pair = (token, documents[doc_id].document_id)\n",
        "              pairs.append(pair)\n",
        "      sorted_pairs = uniq(sorted(pairs, key=lambda x: (x[0].lower(), x[1])))\n",
        "\n",
        "      # build dictionary and inverted index from preprocessed document bodies\n",
        "      global dictionary\n",
        "      dictionary = []\n",
        "      global inverted_index\n",
        "      inverted_index = {}\n",
        "      for term, doc_id in sorted_pairs:\n",
        "        if term not in inverted_index:\n",
        "          inverted_index[term] = []\n",
        "          dictionary.append(term)\n",
        "        inverted_index[term].append(doc_id) \n",
        "\n",
        "      # build token, document pairs from preprocessed document titles\n",
        "      pairs = []\n",
        "      for doc_id in documents:\n",
        "          tokens = documents[doc_id].preprocessed_title\n",
        "          for token in tokens:\n",
        "              pair = (token, documents[doc_id].document_id)\n",
        "              pairs.append(pair)\n",
        "      sorted_pairs = uniq(sorted(pairs, key=lambda x: (x[0].lower(), x[1])))\n",
        "\n",
        "      # build dictionary and inverted index from preprocessed document titles\n",
        "      global dictionary_titles\n",
        "      dictionary_titles = []\n",
        "      global inverted_index_titles\n",
        "      inverted_index_titles = {}\n",
        "      for term, doc_id in sorted_pairs:\n",
        "        if term not in inverted_index_titles:\n",
        "          inverted_index_titles[term] = []\n",
        "          dictionary_titles.append(term)\n",
        "        inverted_index_titles[term].append(doc_id)           \n",
        "\n",
        "      # build token, document pairs from preprocessed query bodies\n",
        "      pairs = []\n",
        "      for query_id in queries:\n",
        "          tokens = queries[query_id].preprocessed_body\n",
        "          for token in tokens:\n",
        "              pair = (token, queries[query_id].query_id) #doc misto doc.document_id\n",
        "              pairs.append(pair)\n",
        "      sorted_pairs = uniq(sorted(pairs, key=lambda x: (x[0].lower(), x[1])))\n",
        "\n",
        "      # build dictionary and inverted index from preprocessed query bodies      \n",
        "      global query_dictionary\n",
        "      query_dictionary = []\n",
        "      global query_inverted_index\n",
        "      query_inverted_index = {}\n",
        "      for term, query_id in sorted_pairs:\n",
        "        if term not in query_inverted_index:\n",
        "          query_inverted_index[term] = []\n",
        "          query_dictionary.append(term)\n",
        "        query_inverted_index[term].append(query_id)     \n",
        "\n",
        "    def build_tf_idf_frames(self):\n",
        "      '''\n",
        "      Builds tfidf dataframes for both preprocessed document bodies and titles\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tfidf_frame : dataframe\n",
        "          Dataframe consisting of tfidf value for each term and document from\n",
        "          preprocessed document bodies.\n",
        "      \n",
        "      tfidf_frame_titles : dataframe\n",
        "          Dataframe consisting of tfidf value for each term and document from\n",
        "          preprocessed document titles.\n",
        "\n",
        "      '''\n",
        "      # calculate average documents body size for special weighting\n",
        "      avg_doc_size = 0\n",
        "      counter = 0\n",
        "      for doc_id in documents:\n",
        "        avg_doc_size += len(documents[doc_id].preprocessed_body)\n",
        "        counter += 1\n",
        "      avg_doc_size = avg_doc_size/counter\n",
        "\n",
        "      # build a data frame of term frequencies for document bodies\n",
        "      global tf_frame\n",
        "      tf_frame = pd.DataFrame()\n",
        "      tf_frame['terms'] = dictionary\n",
        "      for doc_id in self.all_documents:\n",
        "        term_frequency = {}\n",
        "        for term in self.all_documents[doc_id].preprocessed_body:\n",
        "          if term not in term_frequency:\n",
        "            term_frequency[term] = 1\n",
        "          else:\n",
        "            term_frequency[term] += 1\n",
        "        tf_doc_entry = {}\n",
        "        for term in dictionary:\n",
        "          if term in term_frequency:\n",
        "            tf_doc_entry[term] = term_frequency[term]/(term_frequency[term]+4.289*len(documents[doc_id].preprocessed_body)/avg_doc_size)\n",
        "          else:\n",
        "            tf_doc_entry[term] = 0\n",
        "        tf_frame[doc_id] = tf_frame['terms'].map(tf_doc_entry)\n",
        "      tf_frame.set_index('terms', inplace=True)\n",
        "\n",
        "      # calculate N and get document frequencies\n",
        "      N = len(dictionary)\n",
        "      df_dict = {}\n",
        "      for term in dictionary:\n",
        "        if term not in df_dict:\n",
        "          df_dict[term] = len(inverted_index[term])\n",
        "        else:\n",
        "          df_dict[term] = 0\n",
        "\n",
        "      # build idf dictionary\n",
        "      global idf_dict\n",
        "      idf_dict = dict((term, np.log2((N+1)/(df+1)+1.0001)) for term, df in df_dict.items())\n",
        "      # build tfidf dtaframe\n",
        "      global tfidf_frame\n",
        "      tfidf_frame = tf_frame.apply(lambda x: np.log(1.00097+x)*idf_dict[x.name], axis = 1)\n",
        "\n",
        "      # same steps for document titles\n",
        "      global tf_frame_titles\n",
        "      tf_frame_titles = pd.DataFrame()\n",
        "      tf_frame_titles['terms'] = dictionary\n",
        "      for doc_id in self.all_documents:\n",
        "        term_frequency = {}\n",
        "        for term in self.all_documents[doc_id].preprocessed_title:\n",
        "          if term not in term_frequency:\n",
        "            term_frequency[term] = 1\n",
        "          else:\n",
        "            term_frequency[term] += 1\n",
        "        tf_doc_entry = {}\n",
        "        for term in dictionary:\n",
        "          if term in term_frequency:\n",
        "            tf_doc_entry[term] = term_frequency[term]/(term_frequency[term]+4.289*len(documents[doc_id].preprocessed_title)/avg_doc_size)\n",
        "          else:\n",
        "            tf_doc_entry[term] = 0\n",
        "        tf_frame_titles[doc_id] = tf_frame_titles['terms'].map(tf_doc_entry)\n",
        "      tf_frame_titles.set_index('terms', inplace=True)\n",
        "      df_dict_titles = {}\n",
        "      for term in dictionary:\n",
        "        if term not in df_dict_titles:\n",
        "          df_dict_titles[term] = len(inverted_index[term])\n",
        "        else:\n",
        "          df_dict_titles[term] = 0\n",
        "\n",
        "      global idf_dict_titles\n",
        "      idf_dict_titles = dict((term, np.log2((N+1)/(df+1)+1.00097)) for term, df in df_dict_titles.items())\n",
        "      global tfidf_frame_titles\n",
        "      tfidf_frame_titles = tf_frame_titles.apply(lambda x: np.log(1.00097+x)*idf_dict_titles[x.name], axis = 1)  \n",
        "             \n",
        "    def query_to_tfidf(self, query):\n",
        "      '''\n",
        "      Builds a dataframe of tfidf values for provided query\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      query : query object\n",
        "        Provided query.      \n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      query_tfidf : dataframe\n",
        "          Dataframe consisting of tfidf values for provided query.\n",
        "\n",
        "      '''     \n",
        "      terms = queries[query.query_id].preprocessed_body\n",
        "      # build term frequency dataframe\n",
        "      term_frequency = {}\n",
        "      for term in terms:\n",
        "        if term not in term_frequency.keys():\n",
        "          term_frequency[term] = 1\n",
        "        else:\n",
        "          term_frequency[term] += 1\n",
        "      full_term_frequency = {}\n",
        "      for term in dictionary:\n",
        "        if term in term_frequency:\n",
        "          full_term_frequency[term] = term_frequency[term]\n",
        "        else:\n",
        "          full_term_frequency[term] = 0\n",
        "      full_term_frequency = pd.DataFrame.from_dict(full_term_frequency,orient='index',columns=['Query'])\n",
        "      query_tfidf = full_term_frequency.apply(lambda x:  np.log(1.00097+x)*idf_dict[x.name], axis = 1)\n",
        "\n",
        "      return(query_tfidf)\n",
        "\n",
        "    def tfidf_search(self, query):\n",
        "      '''\n",
        "      For provided query finds the most relevant documents\n",
        "\n",
        "      Parameters\n",
        "      -------\n",
        "      query : Query object\n",
        "          A provided query.\n",
        "      \n",
        "      Returns\n",
        "      -------\n",
        "      output_documents : Ordered dict\n",
        "          An ordered dict of Document object sorted by relevance\n",
        "\n",
        "      '''      \n",
        "      # get querie's tfidf values\n",
        "      query_tfidf = self.query_to_tfidf(query)\n",
        "\n",
        "      # build dictionary of similarities to all documents bodies\n",
        "      columns = list(tfidf_frame)\n",
        "      similarity_dict = {}\n",
        "      for column in columns:\n",
        "        similarity = float(cosine_similarity(query_tfidf['Query'].values.reshape(1, -1), tfidf_frame[column].values.reshape(1, -1)))\n",
        "        similarity_dict[column] = similarity     \n",
        "      \n",
        "      # build dictionary of similarities to all document titles\n",
        "      columns = list(tfidf_frame_titles)\n",
        "      similarity_dict_titles = {}\n",
        "      for column in columns:\n",
        "        similarity = float(cosine_similarity(query_tfidf['Query'].values.reshape(1, -1), tfidf_frame_titles[column].values.reshape(1, -1)))\n",
        "        similarity_dict_titles[column] = similarity\n",
        "\n",
        "      # if title similarity is greater than 0.65, use title similarity, otherwise use body similarity\n",
        "      combined_similarity_dict = {}\n",
        "      for key, value in similarity_dict.items():\n",
        "        similarity_titles = similarity_dict_titles[key]\n",
        "        if similarity_titles >= 0.65:\n",
        "          combined_similarity_dict[key] = similarity_titles\n",
        "        else:\n",
        "          combined_similarity_dict[key] = value\n",
        "      combined_similarity_dict = OrderedDict(sorted(combined_similarity_dict.items(), key=lambda x: x[1], reverse=True))    \n",
        "     \n",
        "      # build output document list\n",
        "      output_documents = []\n",
        "      for key, value in combined_similarity_dict.items():\n",
        "        output_documents.append(key)     \n",
        "\n",
        "      return output_documents\n",
        "\n",
        "    def search(self, query):\n",
        "        \"\"\"The ranked retrieval results for a query.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query : Query\n",
        "            A query.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        list of Document\n",
        "            The ranked retrieval results for a query.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        relevant_doclist = self.tfidf_search(query)\n",
        "        result = []\n",
        "        for doc in relevant_doclist:\n",
        "          result.append(documents[doc])\n",
        "        self.output_documents = result\n",
        "\n",
        "        return self.output_documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIe-D0MVX7CG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwrCzoaZhWi4"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bioYHbahybQ"
      },
      "source": [
        "The following code evaluates your information retrieval system using the Mean Average Precision evaluation measure.\n",
        "You can [check out on GitLab](https://gitlab.fi.muni.cz/xstefan3/pv211-utils/blob/master/pv211_utils/eval.py) how Mean Average Precision is computed.\n",
        "\n",
        "If you choose to `submit_result`, the result of your run will appear among our [Leaderboard submissions](https://docs.google.com/spreadsheets/d/e/2PACX-1vSGTg_Agc0SowDIsDDsaBN_UD-9r-F2eSpozyvVA8F51YHt3GmAle3niaCoj0ocazjDm01OJNgNEykZ/pubhtml).\n",
        "\n",
        "Then, your best score for each week will be submited and ranked in the Leaderboard sheet. The best solvers will get small **awards during the semester**, or some **seriously big awards** after the personal check, at the end of the competition (that's the 8th of May for now)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssX-nvxGu3JK"
      },
      "source": [
        "from pv211_utils.eval import mean_average_precision\n",
        "\n",
        "mean_average_precision(MyIRSystem(), submit_result=False, author_name=\"Čapek, Nicholas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmrIiB78nvWM"
      },
      "source": [
        "Please be polite and do not spoil the game for the others ;)\n",
        "\n",
        "**Have fun!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20rKmDny7Z3K"
      },
      "source": [
        "## Bibliography\n",
        "[1] Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. [*Introduction to information retrieval*](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf). Cambridge university press, 2008."
      ]
    }
  ]
}